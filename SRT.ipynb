{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greetings fellow visitor!\n",
    "\n",
    "The very main purpose of the following content is to build a model to predict labels in a review/evaluation/job, we will take advantage of Selenium Webdriver to find, extract and store the data provided by the [Client](https://www.facebook.com/pg/facebook/about/), later we will explore this data, clean it, fill into several training models and aim for the highest accuracy possible.\n",
    "\n",
    "\n",
    "## About the Author\n",
    "<img style=\"float: left; margin-right:10px; margin-left:10px\" src=\"https://scontent.frec5-1.fna.fbcdn.net/v/t1.0-9/15941393_1501633696543698_5959706666543527839_n.jpg?oh=1fc3b1e567c71bdad8cc14e79a4f1392&oe=5973DDA6\">\n",
    "\n",
    "**Eduardo Gordilho** ([Facebook](https://www.facebook.com/edugordilho), [Linkedin](https://www.linkedin.com/in/eduardo-gordilho-99193550/), [GitHub](https://github.com/eduord)),\n",
    "I'm a Civil Engineer with a B.Eng. (5 yrs.) at [UFPE](https://www.ufpe.br/) ([QS Rank](https://www.topuniversities.com/universities/universidade-federal-de-pernambuco-ufpe)) with initial roots on Electric-Electronic Engineering.\n",
    "\n",
    "Recently I've been very interested in data analysis and machine learning, I don't code for living and I don't master any aspects of what's being presented below, but hopefully you'll find this practice project interesting.\n",
    "\n",
    "**<div style=\"text-align: right\">Hope you enjoy it,\n",
    "<br>\n",
    "Eduardo.</div>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic functionality:\n",
    "\n",
    "- Go inside [SRT](https://our.intern.facebook.com/intern/search_team)\n",
    "- Set up your project, save and enqueue and start a rating session.\n",
    "- For each result it'll:\n",
    "   - Check for errors\n",
    "       - Handle errors\n",
    "   - Identify the module\n",
    "       - Gather the data\n",
    "       - Produce DataFrames\n",
    "       - Store DataFrames into local files\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Summary\n",
    "\n",
    "This section will make your navigation through this notebook easier, come back here by CTRL + F (Windows) and type \"Summary\".\n",
    "\n",
    "## [Setting Up](#setting_up)\n",
    "- [Importing Dependencies](#dependencies)\n",
    "- **Possibily Optional**: [Setting Up Cookies](#setting_up) (If it's your 1st time here)\n",
    "    - [Saving Browser Session](#saving_browser_session)\n",
    "- [Start a new session](#start_a_new_session)\n",
    "\n",
    "## [Exploring Functions](#exploring_functions)\n",
    "- [Function: Get all element attributes](#fun_get_all_element_attributes)\n",
    "- [Function: Get all butons as List](#fun_get_all_butons_as_list)\n",
    "- [Function: Get all butons as Dictionary](#fun_get_all_buttons_as_dict)\n",
    "\n",
    "**NOTE**: This is only a section, DOES NOT include all functions contained in this document.\n",
    "\n",
    "## [User Interaction (UI)](#user_interaction)\n",
    "- [Click Button](#click_button)\n",
    "    - [Function: Click Button by String identifier](#fun_click_button)\n",
    "    - [Function: Go to SRT Main Screen](#fun_goto_main_srt_screen)\n",
    "- [Navigating through jobs](#wrapper_fun_jobs_navigation)\n",
    "    - [Goto next job](#fun_next_job)\n",
    "    - [Goto previous job](#fun_previous_job)\n",
    "- [Set Current Experiment by String identifier](#fun_set_current_experiment_to)\n",
    "- [Save and Enqueue](#fun_save_and_enqueue)\n",
    "- [See more content](#fun_see_more)\n",
    "- [Opens a new tab](#fun_open_tab)\n",
    "- [Close error pop](#fun_close_error_popup)\n",
    "\n",
    "## [Detection](#detection)\n",
    "- [Identifying Screen Modules and Errors](#wrapper_identifying)\n",
    "    - [Function: Status](#fun_status)\n",
    "    - [UNFINISHED SECTION: Errors](#errors)\n",
    "\n",
    "## [Data Extraction](#data_extraction):\n",
    "- [Function: Get Main Screen as dict](#fun_get_main_screen_as_dict)\n",
    "- [Function: Get Today Queries as DataFrame](#fun_get_today_queries_info_as_dataframe)\n",
    "- [Function: Get Today Queries as List of Dictionaries](#fun_geT_today_queries_info_as_list_of_dicts)\n",
    "- [Get Jobs info](#wrapper_fun_get_job_info)\n",
    "    - [Function: Get job info as dict](#fun_get_job_info_as_dict)\n",
    "    \n",
    "## [Definitions](#definitions)\n",
    "- [Definition: Status list](#def_status_list)\n",
    "- [Definition: Modules list](#def_modules_list)\n",
    "\n",
    "## [Wrangling](#wrangle)\n",
    "- [Basic Wrangle](#basic_wrangle)\n",
    "    - [Function: UTime convertion](#fun_utime_conversion)\n",
    "    - [Function: Get node text excluding children](#fun_get_text_excluding_children)\n",
    "\n",
    "- [Modules:](#modules_wrangle)\n",
    "    - [Wrangle Page and Group module](#module_page_and_group)\n",
    "\t- [Wrangle Video](#module_video)\n",
    "\t- [Wrangle Profile](#module_profile)\n",
    "\t\t- [Definition: Triggers](#def_triggers)\n",
    "    - [Wrangle Photo](#module_photo)\n",
    "    - [Wrangle External Contentl](#module_external)\n",
    "    - [Wrangle User Content](#module_user_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"dependencies\">Importing Dependencies</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our main source of power\n",
    "from selenium import webdriver\n",
    "# Allow us to perform hotkey interactions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Is used to hover mouse function\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Vectorized operations and display multiple DataFrames on screen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core import display as ICD\n",
    "\n",
    "# Helps with default dict structures\n",
    "import collections\n",
    "# Helps us with time data manipulation\n",
    "import datetime\n",
    "# Track performace when needed\n",
    "import time\n",
    "# To import our cookies and in the future the classifiers\n",
    "import pickle\n",
    "\n",
    "# To help parse selenium image to Pillow\n",
    "import io\n",
    "# To visualize and manipulate images\n",
    "from PIL import Image\n",
    "from PIL import ImageGrab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# To parse profile pictures links to Pillow\n",
    "import urllib\n",
    "\n",
    "# To help with file storage\n",
    "import os\n",
    "# To help us with waiting page to load functions\n",
    "from time import sleep\n",
    "\n",
    "# To help with string manipulation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "# Remove double letters\n",
    "from itertools import groupby\n",
    "# Secret\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"setting_up\">Setting up</a>\n",
    "\n",
    "We will use your cookies to avoid 2-step-verification and additional initial code to log into your account.\n",
    "\n",
    "Therefore the following function we'll be used to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cookies():\n",
    "    pickle.dump(browser.get_cookies() , open(\"SRTCookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"saving_browser_session\">Saving Browser Session</a>\n",
    "\n",
    "The following chunk of code is inteded **if this is your first time** through this code.\n",
    "\n",
    "**ATTENTION**: The chunk bellow was purposefully modified to *Raw NBConvert* to avoid unintentional execution. If you need to go through this step please change the chunk type to *Code*:\n",
    "\n",
    "By either doing this with the cursor:\n",
    "\n",
    "![Convert Raw NBConvert Chunk to Code Chunk](chunk_to_code.png)\n",
    "\n",
    "Or by clicking in the Chunk and then pressing the key **[ESC]** (note that the vertical line that once was green must now be blue) followed by pressing the key for letter **[M]**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://our.intern.facebook.com/intern/search_team')\n",
    "save_cookies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"start_a_new_session\">Start a New Session</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_new_session():\n",
    "    session = webdriver.Chrome(executable_path=\"./chromedriver.exe\")\n",
    "    # Open any website to allow us to load cookies\n",
    "    session.get('https://www.facebook.com/')\n",
    "    # Load cookies\n",
    "    for cookie in pickle.load(open(\"SRTCookies.pkl\", \"rb\")):\n",
    "        session.add_cookie(cookie)\n",
    "    return(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"exploring_functions\">Exploring Functions</a>\n",
    "\n",
    "This section will support basic evaluations, functions here are for the purpose of evaluating some conditions with **WebElements** and **Buttons**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_all_element_attributes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Javascript func, not Python\n",
    "def get_all_element_attributes(element):\n",
    "    return(browser.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_get_all_butons_as_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List with button elements (text,css_xpath,index)\n",
    "def get_all_buttons_as_list():\n",
    "    buttons_list = []\n",
    "        \n",
    "    xpath = '//input[@type=\"button\"]'\n",
    "    [buttons_list.append((e.get_attribute(\"value\"),xpath,i)) for i,e in enumerate(browser.find_elements_by_xpath(xpath))]\n",
    "\n",
    "    xpath = '//a[@role=\"button\" and not(contains(@style,\"display:none\"))]'\n",
    "    [buttons_list.append((e.text,xpath,i)) for i,e in enumerate(browser.find_elements_by_xpath(xpath))]\n",
    "\n",
    "    xpath = '//button'\n",
    "    [buttons_list.append((e.text,xpath,i)) for i,e in enumerate(browser.find_elements_by_xpath(xpath))]\n",
    "    return(buttons_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_get_all_buttons_as_dict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_buttons_as_dict():\n",
    "    buttons_dict = {}\n",
    "    for e in get_all_buttons_as_list():\n",
    "        buttons_dict[e[0]] = {'xpath': e[1],'index':e[2]}\n",
    "    return buttons_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_element_from_dict(parent, dictionary,first_level_only = False, precise_class = False):\n",
    "    \n",
    "    # If dictionary has information about find parameters, use it and ignore what the user is giving as parameters\n",
    "    if 'find_parameters' in dictionary.keys():\n",
    "        parameters = dictionary['find_parameters']\n",
    "        first_level_only = parameters['first_level_only']\n",
    "        precise_class = parameters['precise_class']    \n",
    "\n",
    "    if first_level_only:\n",
    "        depth = '/'\n",
    "    else:\n",
    "        depth = '//'\n",
    "    \n",
    "    if 'class' in dictionary.keys():\n",
    "        if precise_class:\n",
    "            xpath = '.{}{}[@class=\"{}\"]'.format(depth,dictionary['tag'],dictionary['class'])  \n",
    "        else:\n",
    "            xpath = '.{}{}[contains(@class,\"{}\")]'.format(depth,dictionary['tag'],dictionary['class'])\n",
    "    else:\n",
    "        xpath = '.{}{}'.format(depth,dictionary['tag'])\n",
    "        result = parent.find_element_by_xpath(xpath)\n",
    "\n",
    "    return(parent.find_element_by_xpath(xpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_element(parent, tag, class_ = None, first_level_only = False, precise_class = False):\n",
    "    if first_level_only:\n",
    "        depth = '/'\n",
    "    else:\n",
    "        depth = '//'\n",
    "\n",
    "    if class_:\n",
    "        try:\n",
    "            if precise_class:\n",
    "                return(parent.find_element_by_xpath('.{}{}[@class=\"{}\"]'.format(depth,tag,class_)))\n",
    "            else:\n",
    "                return(parent.find_element_by_xpath('.{}{}[contains(@class,\"{}\")]'.format(depth,tag,class_)))\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            return(parent.find_element_by_xpath('.{}{}'.format(depth,tag)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"user_interaction\">User Interaction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"click_button\"> Click button</a>\n",
    "\n",
    "Here we'll define a [click function](#fun_click_button) that will receive a string as input and will attempt to click an element that has that string parsed.\n",
    "\n",
    "If you have problems with this function please check the [*function to get all buttons as list*](#fun_get_all_butons_as_list) and see how the elements are gathered, you may have to change how it gathers e[0] (the text part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_click_button\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def click_button(string_identifier):\n",
    "    for e in get_all_buttons_as_list():\n",
    "        # e[0] - Text\n",
    "        # e[1] - Selector as String\n",
    "        # e[2] - Element index in that browser.find_elements_by_css_selector(css_class_selector)\n",
    "        if string_identifier in e[0]:\n",
    "            # Print Button Clicked\n",
    "            print(e)\n",
    "            browser.find_elements_by_xpath(e[1])[e[2]].click()\n",
    "            return(True)\n",
    "    print(\"Couldn't find button, if you have specified the string correctly please check the function get_all_buttons_as_list() and look for missing button types detection\")\n",
    "    return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_goto_main_srt_screen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def goto_main_srt_screen():\n",
    "    srt_links = [\"https://our.intern.facebook.com/intern/search_team\",\"https://intern.facebook.com/intern/search_team\", \"https://our.cstools.facebook.com/intern/search_team\"]\n",
    "    while status()['screen'] != 'srt_main':\n",
    "        time_waited = 0\n",
    "        maxtime = 10\n",
    "        if browser.current_url not in srt_links:\n",
    "            print(\"It seems like you're not on SRT\")\n",
    "            print(\"Your current url is: {}\".format(browser.current_url))\n",
    "            print(\"Not contained in a predefined list of SRT mirrors\")\n",
    "            print(\"Please make sure to update the mirror to srt_links variable if {} is a SRT Mirror\".format(browser.current_url))\n",
    "            print(\"Redirecting to Main SRT Screen:\")\n",
    "            browser.get(\"https://our.intern.facebook.com/intern/search_team\")\n",
    "            print(\"Done!\")\n",
    "            sleep(1)\n",
    "            time_waited += 1\n",
    "            if time_waited > maxtime:\n",
    "                browser.get(\"https://our.intern.facebook.com/intern/search_team\")\n",
    "                print(\"Trying again\")\n",
    "        else:\n",
    "            return(\"We're already on the Main SRT Screen\")\n",
    "    return(\"Successfully loaded main SRT Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"wrapper_fun_jobs_navigation\"></a>\n",
    "<a name=\"fun_next_job\"></a>\n",
    "<a name=\"fun_previous_job\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_job():\n",
    "    return(browser.find_element_by_xpath('//body[contains(@class,\"UIInternPage\")]').send_keys(Keys.ARROW_RIGHT))\n",
    "def previous_job():\n",
    "    return(browser.find_element_by_xpath('//body[contains(@class,\"UIInternPage\")]').send_keys(Keys.ARROW_LEFT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_set_current_experiment_to\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Receives an string identifier for the project and set the project as current experiment\n",
    "def set_current_experiment_to(string_identifier):\n",
    "    if 'srt_main' in status()['screen']:\n",
    "        # Accessing Experiments Dropdown List Current Text\n",
    "        experiments_button = browser.find_element_by_css_selector(\"._42ft._4jy0._55pi._5vto._55_p._2agf._p._4jy3._517h._51sy\")\n",
    "        if string_identifier in experiments_button.text:\n",
    "            print(\"Seems like we're already evaluating the right experiment: {}\".format(experiments_button.text))\n",
    "        else:\n",
    "            current_project = experiments_button.text\n",
    "            while not(browser.find_elements_by_xpath('//div[contains(@class,\"_6a _6b\") and contains(@class,\"openToggler\")]')):\n",
    "                experiments_button.click()\n",
    "                sleep(1)\n",
    "            # CSS Selector that contains each available experiment\n",
    "            experiments = browser.find_elements_by_css_selector(\"._54nh\") # ._54nh\n",
    "            experiments_list = []\n",
    "            for i,e in enumerate(experiments):\n",
    "                experiments_list.append([i,e.text])\n",
    "            indice = [i for i, s in enumerate(experiments_list) if string_identifier in s[1]]\n",
    "            if len(indice) > 1:\n",
    "                print('Seems like we have more than one of your experiments identified by \"{}\", they are:'.format(string_identifier))\n",
    "                for i in indice:\n",
    "                    print(\"{}\".format(experiments_list[i][1]))\n",
    "                print(\"Please specify accordingly\")\n",
    "            else:\n",
    "                experiments[indice[0]].click()\n",
    "                print(\"Project was: {} \\n\\\n",
    "                Set to: {}\".format(current_project,experiments_button.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_save_and_enqueue\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_and_enqueue():\n",
    "    # Getting all text from buttons in one line\n",
    "    if 'Clear Assignment' in [e[0] for e in get_all_buttons_as_list()]:\n",
    "        print(\"It seems like you have already enqueued\")\n",
    "    else:\n",
    "        click_button(\"Save and Enqueue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_see_more\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def see_more():\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\".see_more_link_inner\").click()\n",
    "        return(1)\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_open_tab\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_tab(link):\n",
    "    browser.execute_script(\"window.open('%s');\"%link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_close_error_popup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def close_error_popup():\n",
    "    if click_button(\"Close\") | click_button(\"OK\"):\n",
    "        print(\"Error Popup Closed\")\n",
    "    else:\n",
    "        print(\"Couldn't find a way to close the error\")\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"detection\">Detection</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"wrapper_identifying\">Identifying Current Screen, Errors and Modules</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section every function to identify in which screen we're at will be defined.\n",
    "\n",
    "Currently I have mapped the following screens:\n",
    "- Main SRT Screen\n",
    "- Rating Screen:\n",
    "    - Each (known) module type screen\n",
    "    - Query switch screen\n",
    "    - Known Errors Screens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status** if a function that will retrieve basic informative data about what's on our screen. \n",
    "\n",
    "Status covers:\n",
    "- Is the page Loading?\n",
    "- Screen type check:\n",
    "    - Are we on main srt screen?\n",
    "    - Are we on the rating screen?\n",
    "        - Is this the switch query screen?\n",
    "- Modules check:\n",
    "    - Is this a user content?\n",
    "    - Is this a Profile\n",
    "    - Is this external content?\n",
    "    - Is this an owned page?\n",
    "    - Is this an unowned page?\n",
    "    - Is this a group?\n",
    "    - Is this the photos module\n",
    "    - Is this the video module\n",
    "        - IS there a SEE MORE button?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ERROR is the only element that is a dict, you can get the error type by\n",
    "\n",
    "``[error_indicator['error'] for error_indicator in status()['status']]`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_status\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modules = ['user_content']\n",
    "def status():\n",
    "    status_dict = {'screen': [], 'status': [], 'module': []}\n",
    "    status = []\n",
    "    # Loading?\n",
    "    try:\n",
    "        if browser.find_element_by_xpath('//div[@class=\"mal _52jv\"]/img'):\n",
    "            status.append(\"loading\")\n",
    "            status_dict['status'].append(\"loading\")\n",
    "        elif browser.find_element_by_xpath('//td[@class=\"_51m-\"]/img'):\n",
    "            status.append(\"loading\")\n",
    "            status_dict['status'].append(\"loading\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Main SRT SCreen\n",
    "    try:\n",
    "        if \"Just Go\" in [e[0] for e in get_all_buttons_as_list()]:\n",
    "            status.append('srt_main')\n",
    "            status_dict['screen'].append(\"srt_main\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if we're in the rating screen\n",
    "    try:\n",
    "        # SRT Experiments wrapper\n",
    "        if browser.find_element_by_xpath('//*[@id=\"srt_renderer_wrapper\"]'):\n",
    "            status.append('rating_screen')\n",
    "            status_dict['screen'].append('rating')\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if is Switch Query screen\n",
    "    try:\n",
    "        if browser.find_element_by_xpath('//div[contains(@class,\"_3a18\")]'):\n",
    "            status.append(\"query_switch\")\n",
    "            status_dict['status'].append(\"query_switch\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Checks for Cannot Load data for job [job_id]\n",
    "    try:\n",
    "        if \"Cannot load\" in browser.find_element_by_xpath('//div[contains(@id,\"u_0_3\")]').text:\n",
    "            # Dict alternative\n",
    "            # error_dict = dict.fromkeys(['error'])\n",
    "            # error_dict['error'] = browser.find_element_by_xpath('//div[contains(@id,\"u_0_3\")]').text\n",
    "            # status_dict['status'].append(error_dict)\n",
    "            error = 'error: ' + browser.find_element_by_xpath('//div[contains(@id,\"u_0_3\")]').text\n",
    "            status_dict['status'].append(error)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # User Content\n",
    "    user_content = ['userContentWrapper',\"fbUserContent\"]\n",
    "    \n",
    "    for identifier in user_content:\n",
    "        try:\n",
    "            if browser.find_element_by_xpath('//div[contains(@class,\"{}\")]'.format(identifier)):\n",
    "                status.append(\"user_content\")\n",
    "                status_dict['module'].append(\"user_content\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Profile\n",
    "    try:\n",
    "        # div[contains(@class,\"_3u1\") profile\n",
    "        if \"EntRegularPersonalUser\" in browser.find_element_by_xpath('//a[contains(@class,\"_ohe\")]').get_attribute(\"data-testid\"):\n",
    "            status.append(\"profile\")\n",
    "            status_dict['module'].append(\"profile\")\n",
    "    except:\n",
    "        pass\n",
    "    # External content\n",
    "    try:\n",
    "        if browser.find_element_by_xpath('//div[contains(@class,\"SearchSimpleSingleReviewRenderer\")]//b'):\n",
    "            status.append(\"external_content\")\n",
    "            status_dict['module'].append(\"external content\")\n",
    "    except:\n",
    "        pass\n",
    "    # Owned Page\n",
    "    try:\n",
    "        if \"EntOwnedPage\" in browser.find_element_by_class_name(\"_ohe\").get_attribute(\"data-testid\"):\n",
    "            status.append(\"owned_page\")\n",
    "            status_dict['module'].append(\"owned_page\")\n",
    "    except:\n",
    "        pass\n",
    "    # Unowned Page\n",
    "    try:\n",
    "        if \"EntUnownedPage\" in browser.find_element_by_class_name(\"_ohe\").get_attribute(\"data-testid\"):\n",
    "            status.append(\"unowned_page\")\n",
    "            status_dict['module'].append(\"unowned_page\")\n",
    "    except:\n",
    "        pass\n",
    "    # Group\n",
    "    try:\n",
    "        if \"EntGroup\" in browser.find_element_by_class_name(\"_ohe\").get_attribute(\"data-testid\"):\n",
    "            status.append(\"group\")\n",
    "            status_dict['module'].append(\"group\")\n",
    "    except:\n",
    "        pass\n",
    "    # Photo\n",
    "    try:   \n",
    "        if browser.find_element_by_css_selector(\"._5t31\"):\n",
    "            status.append(\"photo\")\n",
    "            status_dict['module'].append(\"photo\")\n",
    "    except:\n",
    "        pass\n",
    "    # Video\n",
    "    try:\n",
    "        if 'Videos' in browser.find_element_by_xpath('//div[@class=\"_51cm\"]//span').text:\n",
    "            status.append(\"video\")\n",
    "            status_dict['module'].append(\"video\")\n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    # Check if see more button available\n",
    "    try:\n",
    "        if browser.find_element_by_css_selector(\".see_more_link_inner\"):\n",
    "            status.append('see_more')\n",
    "            status_dict['status'].append(\"see_more\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Error Cannot load data from job\n",
    "    # Handle Something Went Wrong\n",
    "    # Click on Close Button\n",
    "    #browser.find_element_by_xpath(\"//em[contains(.,'Close')]\").click()\n",
    "    return(status_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"data_extraction\">Data Exctraction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_get_main_screen_as_dict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_main_screen_summary_as_dict():\n",
    "    main_summary = {}\n",
    "    if 'srt_main' in status()['screen']:\n",
    "        table_head = [e.text for e in browser.find_elements_by_xpath('//thead/tr/th/span/span')]\n",
    "        table_body = [e.text for e in browser.find_elements_by_xpath(\"//table[@class='uiDataTable mvl _5tss']/tbody/tr/td\")]\n",
    "\n",
    "        for i,e in enumerate(table_head):\n",
    "            main_summary[e] = table_body[i]\n",
    "        return(main_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_get_today_queries_info_as_dataframe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_today_queries_info_as_dataframe():\n",
    "    if 'srt_main' in status()['screen']:\n",
    "        # Get the index of most complete attributes\n",
    "        ideal_header_index = np.array([len(e.get_attribute(\"data-tooltip-content\").splitlines()) for i,e in enumerate(browser.find_elements_by_xpath('//div[@class=\"_4fbq\"]//a'))]).argmax()\n",
    "        # Using class of Today Queries to retrieve a list with its information\n",
    "        today_queries_info_list = [e.get_attribute(\"data-tooltip-content\").splitlines() for e in browser.find_elements_by_xpath('//div[@class=\"_4fbq\"]//a')]\n",
    "        headers = [e[:e.find(\":\")] for i,e in enumerate(today_queries_info_list[ideal_header_index])]\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        for i,e in enumerate(today_queries_info_list):\n",
    "            local_headers = [element[:element.find(\":\")] for element in today_queries_info_list[i]]\n",
    "            for j in range(0,len(e)):\n",
    "                current_information = e[j][e[j].find(\":\")+1:].strip()\n",
    "                df.loc[i,local_headers[j]] = current_information\n",
    "        return(df)\n",
    "    else:\n",
    "        print(\"Apparently we're not on the main SRT Screen\")\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_get_today_queries_info_as_list_of_dicts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_today_queries_info_as_list_of_dicts():\n",
    "    if 'srt_main' in status()['screen']:\n",
    "        today_queries_info_list = [e.get_attribute(\"data-tooltip-content\").splitlines() for e in browser.find_elements_by_xpath('//div[@class=\"_4fbq\"]//a')]\n",
    "\n",
    "        today_queries_list = []\n",
    "        for i in range(0,len(today_queries_info_list)):\n",
    "            local_headers = [e[:e.find(\":\")] for i,e in enumerate(today_queries_info_list[i])]\n",
    "            local_information = [e[e.find(\":\")+1:].strip() for i,e in enumerate(today_queries_info_list[i])]\n",
    "            today_queries_list.append(dict(zip(local_headers,local_information)))\n",
    "        return(today_queries_list)\n",
    "    else:\n",
    "        print(\"Apparently we're not on the main SRT Screen\")\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"wrapper_fun_get_job_info\"></a>\n",
    "<a name=\"fun_get_job_info_as_dict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_job_as_dict():\n",
    "    try:\n",
    "        return(dict(zip([e.text for e in browser.find_elements_by_xpath('//div[@class=\"pas _50f8\"]')],\n",
    "                 [e.text for e in browser.find_elements_by_xpath('//div[@class=\"pas\"]')])))\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"definitions\">Definitions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"def_status_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_list = [\"query_switch\",\"srt_main\",\"loading\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"def_modules_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modules_list = [\"user_content\",\"profile\",\"external_content\",\"owned_page\",\"unowned_page\",\"group\",\"photo\",\"video\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"wrangle\">Wrangle</a>\n",
    "<a name=\"wrangling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"basic_wrangle\">Basic wrangling functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_utime_conversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since utime is available this function can be handy\n",
    "def utime_to_dd_mm_aaaa_hh_mm_ss(unix_timestamp):\n",
    "    return(\n",
    "    datetime.datetime.fromtimestamp(\n",
    "        unix_timestamp\n",
    "    ).strftime('%d-%m-%Y %H:%M:%S')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"wrapper_fun_text_without_children\">\n",
    "<a name=\"fun_get_text_excluding_children\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_excluding_children(driver, element):\n",
    "    return driver.execute_script(\"\"\"\n",
    "var parent = arguments[0];\n",
    "var child = parent.firstChild;\n",
    "var ret = \"\";\n",
    "while(child) {\n",
    "    if (child.nodeType === Node.TEXT_NODE)\n",
    "        ret += child.textContent;\n",
    "    child = child.nextSibling;\n",
    "}\n",
    "return ret;\n",
    "\"\"\", element) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_to_str(string):\n",
    "    string = string.split(',')\n",
    "    return([item.replace('\\n','').strip() for item in string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"modules_wrangle\"> Wrangling Modules</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_page\">Wrangle Page and Group module</a>\n",
    "<a name=\"module_group\"></a>\n",
    "<a name=\"module_page_and_group\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun_page_module\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def page_module():    \n",
    "    page_wrapper = browser.find_element_by_xpath('//div[contains(@class,\"clearfix _zw\")]')\n",
    "    page_element = page_wrapper.find_element_by_xpath('//a[contains(@class,\"_ohe\")]')\n",
    "    pg_module = dict()\n",
    "    pg_module['link'] = page_element.get_attribute('href')\n",
    "    pg_module['type'] = page_element.get_attribute('data-testid').replace('serp_result_link#0@','')\n",
    "    pg_module['page_scaled_image_link'] = page_element.find_element_by_xpath('.//img').get_attribute('src')\n",
    "    header_info_element = page_wrapper.find_element_by_xpath('.//div[contains(@class,\"_zs fwb\")]')\n",
    "    pg_module['title'] = unidecode(header_info_element.find_element_by_xpath('./a').text.strip().lower())\n",
    "    \n",
    "    # Subheader\n",
    "    try:\n",
    "        subheader = page_wrapper.find_element_by_xpath('.//div[contains(@class,\"_pac _dj_\")]')\n",
    "    except:\n",
    "        print(\"Odd, apparently there's no subheader\")\n",
    "    \n",
    "    # Place? Businesss? Athlete? Further entity information\n",
    "    try:\n",
    "        if get_text_excluding_children(browser,subheader) != u\"\":\n",
    "            pg_module['subheader_text'] = get_text_excluding_children(browser,subheader)\n",
    "        pg_module['subheader_type'] = subheader.find_element_by_xpath('./span').text #subheader.\n",
    "        pg_module['subheader_info'] = subheader.find_element_by_xpath('./a').text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Ratings and Reviews information\n",
    "    try:\n",
    "        ratings = subheader.find_element_by_xpath('.//div[contains(@class,\"mrs _4sk4\")]')\n",
    "        pg_module['rating_stars'] = ratings.find_element_by_xpath('.//div[contains(@class,\"_6a _2b87\")]').text\n",
    "        pg_module['#ratings'] = ratings.find_element_by_xpath('.//div[contains(@class,\"_6a _2h6i\")]').text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Description\n",
    "    try:\n",
    "        description = page_wrapper.find_element_by_xpath('.//div[contains(@class,\"_946\")]')\n",
    "        # Snippets\n",
    "        pg_module['description'] = description.find_element_by_xpath('.//div[contains(@class,\"_52eh\")]').text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Like = Don't like, Liked = Have liked.\n",
    "    try: \n",
    "        like_info = page_wrapper.find_element_by_xpath('.//button[contains(@class,\"PageLikeButton\")]').text\n",
    "        if like_info == u\"Liked\":\n",
    "            pg_module['likes'] = True\n",
    "        elif like_info == u\"Like\":\n",
    "            pg_module['likes'] = False\n",
    "        else:\n",
    "            print(\"Something went wrong when evaluating if the user likes the page\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Joined Group?\n",
    "    try:\n",
    "        joined_text = page_wrapper.find_element_by_xpath('.//a[contains(@data-bt,\"join_group\")]').text\n",
    "        if joined_text == \"Joined\":\n",
    "            pg_module['joined'] = True\n",
    "        elif joined_text == \"Join\":\n",
    "            pg_module['joined'] = False\n",
    "        else:\n",
    "            print(\"Something went wrong when attempting to detect relationship between searcher and group\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Friends like group? Snippets Information\n",
    "    try:\n",
    "        snippets = dict(zip(['snippet_1','snippet_2'],[unidecode(info.text.lower()).strip() for info in browser.find_elements_by_xpath('//div[contains(@class,\"_ajw\")]')]))\n",
    "        snippets_information = list(snippets.values())\n",
    "        friends_like_this = True in ['other friends like this' in information for information in snippets_information]\n",
    "        if friends_like_this:\n",
    "            pg_module['friends_like_page'] = True\n",
    "        else:\n",
    "            pg_module['friends_like_page'] = False\n",
    "        pg_module['snippets_information'] = [snippets_information]\n",
    "    except:\n",
    "        pass\n",
    "    # page_df = pd.DataFrame(pg_module)\n",
    "    return pd.DataFrame(pg_module,index=['information']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Group Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_video\">Wrangling Video Module</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def video_module():\n",
    "    video_wrapper = browser.find_element_by_xpath('//div[@class=\"_14au\"]')\n",
    "    video_container = video_wrapper.find_element_by_xpath('.//div[@class=\"_2uah\"]')\n",
    "    header_wrapper = video_wrapper.find_element_by_xpath('.//div[@class=\"_2_l3\"]')\n",
    "    title = header_wrapper.find_element_by_xpath('.//div[@class=\"_14az\"]').text\n",
    "    try:\n",
    "        subtitle = header_wrapper.find_element_by_xpath('.//div[contains(@class,\"_2_k_\")]').text\n",
    "    except:\n",
    "        subtitle = np.nan\n",
    "        pass\n",
    "    # Chunk_2[0] Identifier, Chunk_2[1] Related entity\n",
    "    chunk_2_wrapper = video_wrapper.find_element_by_xpath('.//div[@class=\"_42bz _2uab\"]')\n",
    "    chunk_2 = chunk_2_wrapper.find_elements_by_xpath('.//a')\n",
    "    author = {chunk_2[0].text.encode('utf-8'): chunk_2[1].text}\n",
    "    if 'By' in chunk_2[0].text:\n",
    "        author = chunk_2[1].text\n",
    "    author_webelement = chunk_2_wrapper.find_element_by_xpath('.//a[@href != \"#\"]')\n",
    "    author_link = author_webelement.get_attribute('href')\n",
    "    author_type = author_webelement.get_attribute('data-testid').replace('serp_result_link#0@','')\n",
    "    chunk_3 = video_wrapper.find_element_by_xpath('.//div[@class=\"_42b-\"]')\n",
    "    date = float(chunk_3.find_element_by_xpath('.//abbr[\"data-utime\"]').get_attribute(\"data-utime\"))\n",
    "    views = get_text_excluding_children(browser,chunk_3.find_element_by_xpath('.//div[@class=\"fsm fwn fcg\"]'))\n",
    "    video_preview_img = video_container.find_element_by_xpath('.//img').get_attribute('src')\n",
    "    video_link = video_container.find_element_by_xpath('.//a').get_attribute('href')\n",
    "    video_df = pd.DataFrame([title,subtitle,author,author_type,author_link,date,views,video_link,video_preview_img],\n",
    "                 index=['title','subtitle','author','author_type','author_link','date','views','video_link','video_preview_img']).transpose()\n",
    "    return video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_profile\">Wrangle Profile Module</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Magic\" triggers\n",
    "\n",
    "- \"Lives in [where]\n",
    "- \"Read [what]\n",
    "- \"Listens to [what]\n",
    "- \"Lives in [where] (City)\n",
    "    - [where] (State)\n",
    "        - [where] (Country)\n",
    "- \"In a relationship with [person]\n",
    "    - since [full_month],[dd],[aaaa]\n",
    "- \"Married to [person_name]\n",
    "    - since [full_month],[dd],[aaaa]\n",
    "- \"Went to [where]\n",
    "- \"Studies [what] at [where]\n",
    "- \"Studied [what] at [ where]\n",
    "- \"Worked at [where]\n",
    "- \"Works at [where]\n",
    "- [marital_status] Â· [gender] \n",
    "- 1 mutual friend: [name]\n",
    "- [integer] mutual friends: [list]\n",
    "- [integer] followers\n",
    "Extra (found harder):\n",
    "- \"Former\" [proffesion] \n",
    "    - at [place]\n",
    "    \n",
    "    \n",
    "The function bellow should return what information the profile you're looking at contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"def_triggers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triggers = {'read': [\"Read\"],\n",
    " 'work': [\"Works\",\"Worked\"],\n",
    " 'mutual_friends': [\"mutual\"],\n",
    " #'other': ['including','and']\n",
    " 'academic': [\"Studies\",\"Studied\"],\n",
    " 'location': [\"Lives\", \"Went\",\"From\"],\n",
    " 'age': [\"years old\"],\n",
    " 'marital_status': [\"Married\",\"Engaged\",\"relationship\"],\n",
    " 'followers': [\"followers\"],\n",
    " 'musical_taste': [\"Listens\"],\n",
    " 'sexual_interest': [\"Interested\"]}\n",
    "\n",
    "triggers_values = {\n",
    " 'gender': ['Male', 'Female'],\n",
    " 'marital_status': ['Single'],\n",
    " 'mutual_friends': ['including','and'],}\n",
    "#'sexual_interest': ['males', 'females'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\" at \", \" to \", \" in \",\" a \", \" and \", \" em \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(string):\n",
    "    for stop_word in stop_words:\n",
    "        try:\n",
    "            match_index = string.index(stop_word)\n",
    "            string = string[0:match_index] + string[match_index+len(stop_word):]\n",
    "        except:\n",
    "            pass\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering Profile Information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Raw Entities involved\n",
    "raw_entities = [(e.text,get_all_element_attributes(e)) for e in browser.find_elements_by_xpath( \n",
    "    '//div[contains(@data-bt,\"ct\")]//*[not(*) and text()]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def profile_module_info():\n",
    "    # Profile Info\n",
    "    author = {}\n",
    "    author['entity'] = unidecode(get_text_excluding_children(browser,browser.find_element_by_xpath(\"//div[contains(@class,'fwb')]/a\")).lower()).strip()\n",
    "    try:\n",
    "        author['alt_name'] = unidecode(get_text_excluding_children(browser,browser.find_element_by_xpath(\"//div[contains(@class,'fwb')]/a/span\")).lower()).strip()\n",
    "        replace_filter = ['(',')']\n",
    "        for flt in replace_filter:\n",
    "            if flt in author['alt_name']:\n",
    "                author['alt_name'] = author['alt_name'].replace(flt,'')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    author['link'] = [e.get_attribute(\"href\") for e in browser.find_elements_by_xpath\n",
    "                      ('//div[contains(@class,\"clearfix\")]//a[contains(@class,\"_ohe\")]')]\n",
    "\n",
    "    author['picture'] = [e.get_attribute(\"src\") for e in browser.find_elements_by_xpath\n",
    "                         ('//div[contains(@class,\"clearfix\")]//a//img')]\n",
    "    try:\n",
    "        entity_type = browser.find_element_by_xpath(\"//div[contains(@class,'fwb')]/a\").get_attribute(\"data-testid\")\n",
    "        entity_type = entity_type[entity_type.index('@')+1:]\n",
    "        author['entity_type'] = entity_type\n",
    "    except:\n",
    "        pass\n",
    "    author['indicator'] = 'author'\n",
    "    \n",
    "    # Snippet: Hard Part\n",
    "    snippet = []\n",
    "    for info_chunk in browser.find_elements_by_xpath('//div[contains(@class,\"_52eh\")]'):\n",
    "        words = word_tokenize(get_text_excluding_children(browser,info_chunk))\n",
    "        # Find single information inside chunk\n",
    "        try:\n",
    "            for key in triggers.keys():\n",
    "                for trigger in triggers[key]:\n",
    "                    info = {}\n",
    "                    for word in words:\n",
    "                        if trigger == word:\n",
    "                            if key in info.keys():\n",
    "                                info[key] = list(np.append(info[key],\n",
    "                                                           remove_stop_words(info_chunk.text.replace(trigger,'')))).strip()\n",
    "                            else:\n",
    "                                info[key] = remove_stop_words(info_chunk.text.replace(trigger,'')).strip()\n",
    "                            if get_text_excluding_children(browser,info_chunk) != u\"\":\n",
    "                                info['indicator'] = remove_stop_words(get_text_excluding_children(browser,info_chunk)).lower().strip()\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # If we don't have only a value on the multinfo chunk but also an indicator\n",
    "            # such as 'years old', we must split it differently to handle the value\n",
    "            splitted_chunk = [segments.strip() for segments in info_chunk.text.split(u'\\xb7')]\n",
    "            for words in splitted_chunk:\n",
    "                for key in triggers.keys():\n",
    "                    for trigger in triggers[key]:\n",
    "                        info = {}\n",
    "                        if trigger in words:\n",
    "                            if key in info.keys():\n",
    "                                info[key] = list(np.append(info[key],\n",
    "                                                           remove_stop_words(words.replace(trigger,'')))).strip()\n",
    "                            else:\n",
    "                                info[key] = remove_stop_words(words.replace(trigger,'')).strip()\n",
    "                            if get_text_excluding_children(browser,info_chunk) != u\"\":\n",
    "                                info['entity'] = unidecode(author['entity'].lower()).strip()\n",
    "                                snippet.append(info)\n",
    "                # If the chunk is a multilevel information and it doesn't provide an indicator\n",
    "                # Just a single information like: \"Single\", \"Female\", this chunk will handle it\n",
    "                try:\n",
    "                    for key in triggers_values.keys():\n",
    "                        for trigger in triggers_values[key]:\n",
    "                            info = {}\n",
    "                            if trigger in words:\n",
    "                                if key in info.keys():\n",
    "                                    info[key] = list(np.append(info[key],words.replace(trigger,'').strip())).lower()\n",
    "                                else:\n",
    "                                    info[key] = trigger.strip().lower()\n",
    "                                info['entity'] = unidecode(author['entity'].lower()).strip()\n",
    "                                snippet.append(info)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Finds link in chunks of info\n",
    "        try:\n",
    "            # If there's links\n",
    "            link_elements = info_chunk.find_elements_by_xpath('.//a')\n",
    "            for link_element in link_elements:\n",
    "                info = {}\n",
    "                info['link'] = link_element.get_attribute('href')\n",
    "                try:\n",
    "                    entity_type = link_element.get_attribute(\"data-testid\")\n",
    "                    entity_type = entity_type[entity_type.index('@')+1:]\n",
    "                    info['entity_type'] = entity_type\n",
    "                    info['entity'] = unidecode(get_text_excluding_children(browser,link_element).lower()).strip()\n",
    "                    info['indicator'] = remove_stop_words(get_text_excluding_children(browser,link_element.find_element_by_xpath('..'))).lower().strip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        snippet.append(info)\n",
    "    snippet_df = pd.concat([pd.DataFrame(author),pd.DataFrame(snippet)],axis=0).groupby(['link']).last().reset_index()\n",
    "    return(snippet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def profile_subheader():\n",
    "    # Subheader\n",
    "    info = []\n",
    "    subheader = {}\n",
    "    subheader['information'] = [unidecode(get_text_excluding_children(browser,x).lower()).strip() for x in browser.find_elements_by_xpath('//div[contains(@class,\"_pac _dj_\")]//*') if get_text_excluding_children(browser,x) != '']\n",
    "    subheader['indicator'] = [unidecode(get_text_excluding_children(browser,x).lower()).strip() for x in browser.find_elements_by_xpath('//div[contains(@class,\"_pac _dj_\")]')][0]\n",
    "    info.append(subheader)\n",
    "    return(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def profile_module():\n",
    "    return(pd.concat([profile_module_info()],axis=0).groupby('entity').first().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering Relationship\n",
    "\n",
    "Covers:\n",
    "\n",
    "- Is friends?\n",
    "- Is follower?\n",
    "- Has shared?\n",
    "- Have liked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_relationship(): \n",
    "    relationship = {}\n",
    "    try:\n",
    "        # Entity\n",
    "        index = get_text_excluding_children(browser,browser.find_element_by_xpath(\"//div[contains(@class,'fwb')]/a\"))\n",
    "    except:\n",
    "        # No entity found\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        relationship_element = browser.find_element_by_xpath('//div[contains(@class,\"_51xa _cme\")]')\n",
    "    except:\n",
    "        #not a profile\n",
    "        pass\n",
    "    \n",
    "    # Check if is friends\n",
    "    try:\n",
    "        if browser.find_element_by_css_selector(\".FriendButton\").text == u'Friends':\n",
    "            relationship['friends'] = True\n",
    "        elif browser.find_element_by_css_selector(\".FriendButton\").text == u'Add Friend':\n",
    "            relationship['friends'] = False\n",
    "    except:\n",
    "        relationship['friends'] = None\n",
    "    \n",
    "    # Check if there's a friend request\n",
    "    try:\n",
    "        if browser.find_element_by_css_selector(\".FriendButton\").text == u\"Respond to Friend Request\":\n",
    "            relationship['received_friend_request'] = True\n",
    "        if browser.find_element_by_css_selector(\".FriendButton\").text == u\"Friend Request Sent\":\n",
    "            relationship['sent_friend_request'] = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Check if has mutual friends\n",
    "    relationship['share_mutual_friends'] = False\n",
    "    try:\n",
    "        for element in browser.find_elements_by_xpath('//div[contains(@class,\"_52eh\")]'):\n",
    "            if 'mutual friend' in element.text:\n",
    "                relationship['share_mutual_friends'] = True\n",
    "        if 'mutual friend' in browser.find_element_by_xpath('//div[contains(@class,\"_pac _dj_\")]').text:\n",
    "            relationship['share_mutual_friends'] = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Checks if there's follow info:\n",
    "    try:\n",
    "        following_element = relationship_element.find_element_by_xpath('.//button[contains(@class,\"FollowButton\")]')\n",
    "        if \"Following\" in following_element.text:\n",
    "            relationship['following'] = True\n",
    "        elif \"Folow\" in following_element.text:\n",
    "             relationship['following'] = False\n",
    "        else:\n",
    "            print(\"Something unexpected happened to the check following function\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Check if user has shared\n",
    "    try:\n",
    "        if 'Share' in browser.find_element_by_class_name('_4qba').get_attribute('data-intl-translation'):\n",
    "            relationship['shared'] = True\n",
    "        else:\n",
    "            relationship['shared'] = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if user have liked page\n",
    "    try:\n",
    "        if browser.find_element_by_class_name('_48-k').get_attribute('data-testid') == u'fb-ufi-likelink':\n",
    "            relationship['liked'] = False\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if browser.find_element_by_class_name('_48-k').get_attribute('data-testid') == u'fb-ufi-unlikelink':\n",
    "            relationship['liked'] = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if u'Liked' in browser.find_element_by_class_name('PageLikeButton').text:\n",
    "            relationship['liked'] = True\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if u'Like' in browser.find_element_by_class_name('PageLikeButton').text:\n",
    "            relationship['liked'] = False\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Check if user have liked content\n",
    "    try:\n",
    "        if 'unlike' in browser.find_element_by_xpath('//a[contains(@class,\"UFILikeLink\")]').get_attribute(\"data-testid\"):\n",
    "            relationship['liked'] = True\n",
    "        else:\n",
    "            relationship['liked'] = False\n",
    "    except:\n",
    "        pass\n",
    "    return(relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_photo\">Wrangle Photos module</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying link of the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def photo_module():\n",
    "    photo = {}\n",
    "    photo_element = browser.find_element_by_xpath('//a[@class=\"_5t31\"]')\n",
    "    photo['photo_link'] = photo_element.get_attribute('href')\n",
    "    scaled_element = photo_element.find_element_by_xpath('.//img')\n",
    "    photo['scaled_link'] = scaled_element.get_attribute('src')\n",
    "    photo['alt_text'] = scaled_element.get_attribute('alt')\n",
    "    #Clicking to see content\n",
    "    photo_element.click()\n",
    "    \n",
    "    # INSIDE THE PHOTO\n",
    "    \n",
    "    # Is there a See more button in the content? Better try to click it if so\n",
    "    see_more()\n",
    "    \n",
    "    # main_wrapper = browser.find_element_by_xpath('//body//div[@class=\"clearfix fbPhotoSnowliftPopup\"]')\n",
    "    # Wrapper containing all photo theater elements\n",
    "    main_wrapper = browser.find_element_by_xpath('//body//div[@class=\"fbPhotoSnowliftContainer snowliftPayloadRoot uiContextualLayerParent\"]//div[@class=\"clearfix fbPhotoSnowliftPopup\"]')\n",
    "    # Right part (author, comments, etc.)\n",
    "    right_POI_wrapper = main_wrapper.find_element_by_xpath('.//div[@class=\"uiScrollableAreaWrap scrollable\"]')\n",
    "    # Left part (picture)\n",
    "    left_POI_wrapper = main_wrapper.find_element_by_xpath('.//div[@class=\"stageWrapper lfloat _ohe\"]')\n",
    "\n",
    "    #post_privacy'//div[@id=\"fbPhotoSnowliftAudienceSelector\"]/div[@class=\"mbs fbPhotosAudienceContainerNotEditable]/a'\n",
    "    photo['post_privacy'] = browser.find_element_by_xpath('//div[contains(@class,\"PhotosAudienceContainerNotEditable\")]/a').get_attribute('data-tooltip-content')\n",
    "    photo['author_name'] = right_POI_wrapper.find_element_by_xpath('//div[contains(@class,\"fbPhotoContributorName\")]/a').text\n",
    "    photo['link_profile_mini_pic'] = browser.find_element_by_xpath('//img[contains(@class,\"_s0 _5xib _5sq7 _44ma _rw img\")]').get_attribute('src')\n",
    "    photo['photo_utime'] = browser.find_element_by_tag_name('abbr').get_attribute('data-utime')\n",
    "    photo['photo_text'] = browser.find_element_by_css_selector(\".hasCaption\").text\n",
    "    photo_info_webelement = left_POI_wrapper.find_element_by_xpath('//img[contains(@class,\"spotlight\")]')\n",
    "    photo['photo_link'] =  photo_info_webelement.get_attribute('src')\n",
    "    photo['computer_vision'] = photo_info_webelement.get_attribute('alt')\n",
    "\n",
    "    photo['photo_text'] = right_POI_wrapper.find_element_by_xpath('.//span[contains(@class,\"fbPhotosPhotoCaption\")]').text\n",
    "\n",
    "    # Try to gather Tagged people if there's any\n",
    "    try:\n",
    "        photo['tag_list'] = right_POI_wrapper.find_element_by_xpath('.//span[@class=\"fbPhotoTagList\"]/span')\n",
    "        photo['tag_items'] = tag_list.find_elements_by_xpath('.//a[@class=\"taggee\"]')\n",
    "        photo['tag_name_n_link_tuples'] = [(items.text,items.get_attribute(\"href\")) for items in tag_items]\n",
    "    except:\n",
    "        print(\"No taggs found\")\n",
    "    \n",
    "    # Defining Photo Df\n",
    "    photo_df = pd.DataFrame(photo,index=['photo'])\n",
    "    \n",
    "    # Gathering comments\n",
    "    # First check if there's a \"View More Comments\" button\n",
    "    try:\n",
    "        right_POI_wrapper.find_element_by_xpath('.//a[@class=\"UFIPagerLink\"]').click()\n",
    "    except:\n",
    "        print(\"\"\"Sems like there's no \"See More Comments\" button\"\"\")\n",
    "        \n",
    "    # Extracting comments          \n",
    "    try:\n",
    "        print('hi')\n",
    "        comments_element = right_POI_wrapper.find_elements_by_xpath('.//div[@class=\"UFICommentContentBlock\"]')\n",
    "        comments_authors_element_list = [comments_element.find_element_by_xpath('.//a[contains(@class,\"UFICommentActorName\")]') for comment in comments_element]\n",
    "        comments_name = []\n",
    "        comments_link = []\n",
    "        print('hi')\n",
    "        comments_name_and_link = [(comments_name.append(author.text),comments_link.append(author.get_attribute(\"href\"))) for author in comments_authors]\n",
    "        comments_timestamp_elements = [comments_element.find_element_by_xpath('.//abbr[@class=\"livetimestamp\"]') for comment in comments_element]\n",
    "        comments_timestamp = [timestamp.get_attribute(\"data-utime\") for timestamp in comments_timestamp_elements]\n",
    "        comments_text_elements = [comments_element.find_element_by_xpath('.//span[contains(@class,\"UFICommentBody\")]') for comment in comments_element]\n",
    "        comments_text = [comments_element.text for comment in comments_text_elements]\n",
    "        \n",
    "        # Remember to add extra code here to identify the level of the comment (parent comment and child comments)\n",
    "        # comments_text_parent = [comment.parent for comment in comments_text_elements]\n",
    "        comments_df = pd.DataFrame([comments_text,comments_name,comments_link,comments_timestamp],index=['comment','author_name','author_link','utime'])\n",
    "    except:\n",
    "        print(\"No comments found\")\n",
    "    return photo_df,comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming content is avaliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define how to gather:\n",
    "- Post Privacy information\n",
    "- Author Name\n",
    "- Link to the mini picture of the author of the photo post\n",
    "- Utime of the photo post\n",
    "- Photo Post:\n",
    "    - Photo Link\n",
    "    - Computer vision description\n",
    "    - Photo Text\n",
    "    - Tagged People\n",
    "    - Comments\n",
    "        - Checks if there's a see more button and click it if there is\n",
    "        - Author\n",
    "        - Time\n",
    "        - Profile link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to associate each comment to a parent comment, check for nested comments.\n",
    "Maybe create a list of comments in comments \n",
    "or a list of comments that have comments on it and for each one the collection of comments inside, IDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "photo_df = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing DataFrame with gathered information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Theater Mode after gathering all information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Close Photo Module Button\n",
    "browser.find_element_by_xpath('//a[contains(@data-testid,\"snowliftclose\")]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_external\">Wrangle External Module</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"module_user_content\">Wrangling User Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_content():\n",
    "    try:\n",
    "        main_wrapper = browser.find_element_by_xpath('//div[contains(@class,\"userContentWrapper\")]')\n",
    "    except:\n",
    "        main_wrapper = browser.find_element_by_xpath('//div[contains(@class,fbUserContent)]')\n",
    "    # Up: Content part (author name,text, original author name...)\n",
    "    # Down: Relationship part(like,comment,share)\n",
    "    up_down_division = main_wrapper.find_elements_by_xpath('.//div')\n",
    "    content_part = up_down_division[0]\n",
    "    relationship_part = up_down_division[1]\n",
    "    uc_df = {}\n",
    "    # Header description part\n",
    "    uc_df['header'] = content_part.find_element_by_xpath('.//div[contains(@class,\"_52eh\")]').text\n",
    "    # Author Part Header\n",
    "    try:\n",
    "        author_element = content_part.find_element_by_xpath('.//div[contains(@class,\"_5x46\")]//div[contains(@class,\"clearfix\")]')\n",
    "    except:\n",
    "        try:\n",
    "            author_element = content_part.find_element_by_xpath('.//div[contains(@class,\"_5x46\")]')\n",
    "        except:\n",
    "            print(\"Something went wrong when trying to identify the author of this user content\")\n",
    "            pass\n",
    "    uc_df['author_link'] = author_element.find_element_by_xpath('.//a').get_attribute('href')\n",
    "    uc_df['author_mini_pic'] = author_element.find_element_by_xpath('.//img').get_attribute('src')\n",
    "    author_title_element = content_part.find_element_by_xpath('.//h5')\n",
    "   \n",
    "    try:\n",
    "        if get_text_excluding_children(browser,author_title_element) != u\"\":\n",
    "            uc_df['author_name'] =  unidecode(get_text_excluding_children(browser,author_title_element).lower()).strip()\n",
    "        else:\n",
    "            uc_df['author_name'] = unidecode(author_title_element.find_element_by_xpath('.//a').text.lower()).strip()\n",
    "    except:\n",
    "        print(\"Couldn't determine author title/name\")\n",
    "    \n",
    "    # If there's any author text:\n",
    "    try:\n",
    "        author_text_part = content_part.find_element_by_xpath('.//div[contains(@class,\"_5pbx userContent\")]')\n",
    "        uc_df['author_text'] = author_text_part.find_element_by_xpath('.//p').text\n",
    "        # here we can have /div class text_exposed_root where there's a paragraph <p> inside with text\n",
    "        # if the text is greater than a certain number of characters there'll be a Countinue Reading\n",
    "            # Continue Reading is inside text_exposed_root in a <span> of class text_exposed_link, you can get the original link\n",
    "            # in the href\n",
    "        # Entire text or partial text? (Continue Reading available?)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Post date and privacy information\n",
    "    chunk_3 = content_part.find_element_by_xpath('.//div[contains(@class,\"_5pcp _5lel\")]')\n",
    "    uc_df['time'] = chunk_3.find_element_by_xpath('.//abbr[@data-utime]').get_attribute('data-utime')\n",
    "    try:\n",
    "        uc_df['privacy'] = chunk_3.find_element_by_xpath('.//a[contains(@class,\"Privacy\")]').get_attribute('aria-label')\n",
    "    except:\n",
    "        try:\n",
    "            uc_df['privacy'] = chunk_3.find_element_by_xpath('.//a[@data-tooltip-content]').get_attribute('data-tooltip-content')\n",
    "        except:\n",
    "            print(\"Something went wrong when trying to extract the privacy information\")\n",
    "    \n",
    "    # Action type\n",
    "    try:\n",
    "        author_info = author_element.find_element_by_xpath('.//span[@class=\"fcg\"]')\n",
    "        uc_df['author_action_type'] = get_text_excluding_children(browser,author_info) # Shared, Added\n",
    "        action_complement_element = author_element.find_element_by_xpath('.//a')\n",
    "        action_complement = {}\n",
    "        action_complement['link'] = action_complement_element.get_attribute('href')\n",
    "        action_complement['complement'] = action_complement_element.text\n",
    "        uc_df['author_action_complement'] = [action_complement]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Other entities involved?\n",
    "    try:\n",
    "        authors_involved_elements = author_info.find_elements_by_xpath('.//a[contains(@class,\"profileLink\")]')[1:]\n",
    "        for author in authors_involved_elements:\n",
    "            # involved[i]: [name,link]\n",
    "            authors_involved['title'].append(get_text_excluding_children(browser,author))\n",
    "            authors_involved['link'].append(author.get_attribute('href'))\n",
    "        if authors_involved['title'] or authors_involed['link']:\n",
    "            uc_df['authors_involved'] = authors_involved\n",
    "        print(10*'-')\n",
    "        print(uc_df['authors_involved'])\n",
    "        print(10*'-')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # This should probably be re-located inside the previous chunk of code\n",
    "    shared_content_wrapper = content_part.find_element_by_xpath('.//div[contains(@class,\"_3x-2\")]')\n",
    "    try:\n",
    "        # Type Re-share rom someone\n",
    "        # Shared Wapper also _3x-2\n",
    "        shared_content_wrapper = content_part.find_element_by_xpath('.//div[contains(@class,\"clearfix mtm\")]')\n",
    "        uc_df['original_author_content_link'] = shared_content_wrapper.find_element_by_xpath('./a').get_attribute('href')\n",
    "        uc_df['original_author_mini_pic'] = shared_content_wrapper.find_element_by_xpath('.//img').get_attribute('src')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        #Type Reshare text with link\n",
    "        shared_content_wrapper = content_part.find_element_by_xpath('.//div[contains(@class,\"_5cq3\")]')\n",
    "        # Content linkfwn fcg\n",
    "        uc_df['original_author_content_link'] = shared_content_wrapper.find_element_by_xpath('.//a').get_attribute('href')\n",
    "        uc_df['original_content_image_preview'] = shared_content_wrapper.find_element_by_xpath('.//img').get_attribute('src')\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    try:\n",
    "        shared_content_wrapper = content_part.find_element_by_xpath('.//div[contains(@class,\"clearfix mtm\")]')\n",
    "        uc_df['original_author_content_link'] = shared_content_wrapper.find_element_by_xpath('./a').get_attribute('href')\n",
    "    except:\n",
    "        pass\n",
    "               \n",
    "    #Video\n",
    "    try:\n",
    "        uc_df['shared_content_wrapper'] = content_part.find_element_by_xpath('.//div[contains(@class,\"_150c\")]')\n",
    "        uc_df['original_author_content_img'] = shared_content_wrapper.find_element_by_xpath('.//a').get_attribute('src')\n",
    "    except:\n",
    "        pass\n",
    "                    \n",
    "    # Trying to determine original information if it's a video\n",
    "    try:\n",
    "        original_content_part = content_part.find_element_by_xpath('.//div[contains(@class,\"mtm\")]')\n",
    "        original_author_element = original_content_part.find_element_by_xpath('.//span[contains(@class,\"fwn fcg\")]')\n",
    "        uc_df['original_author_title'] = original_author_element.find_element_by_xpath('//a').text\n",
    "        uc_df['original_author_link'] = original_author_element.find_element_by_xpath('.//a').get_attribute('href')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Shared a movie\n",
    "    try:\n",
    "        original_author_wrapper = original_content_part.find_element_by_xpath('.//div[contains(@class,\"_4sdm _6lh _dcs\")]')\n",
    "        original_author_element = original_author_wrapper.find_elements_by_xpath('.//div[contains(@class,\"_6lp\")]/*')\n",
    "        uc_df['original_author_title'] = original_author_element[0].text # find_element_by_xpath('.//a').text\n",
    "        uc_df['original_author_link_element'] = original_author_element[0].find_element_by_xpath('.//a')\n",
    "        uc_df['original_author_link'] = original_author_content_link = original_author_link_element.get_attribute('href')\n",
    "        uc_df['original_author_type'] = original_author_element[1].text     \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # External Link\n",
    "    try:\n",
    "        image_elements = original_content_part.find_elements_by_xpath('.//img')\n",
    "        images = []\n",
    "        for element in image_elements:\n",
    "            image = {}\n",
    "            image['alt'] = element.get_attribute('alt')\n",
    "            image['link'] = element.get_attribute('src')\n",
    "            images.append(image)\n",
    "\n",
    "        uc_df['shared_img'] = [images]\n",
    "        uc_df['shared_link'] = original_content_part.find_element_by_xpath('.//div[contains(@class,\"_3ekx _29_4\")]//a').get_attribute('href')\n",
    "        info_wrapper = original_content_part.find_element_by_xpath('.//div[contains(@class,\"_6m3 _--6\")]')\n",
    "        uc_df['description'] = info_wrapper.find_element_by_xpath('.//div[contains(@class,\"_6m7 _3bt9\")]').text\n",
    "        uc_df['shared_website_title'] = info_wrapper.find_element_by_xpath('.//div[contains(@class,\"_6lz _6mb ellipsis\")]').text\n",
    "        uc_df['headline'] = info_wrapper.find_element_by_xpath('.//div[contains(@class,\"mbs _6m6 _2cnj _5s6c\")]//a').text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Blue gray checkmarks\n",
    "    try:\n",
    "        #//*[@id=\"u_6g_b\"]\n",
    "        # RnA = Reputability and Authenticity\n",
    "        original_author_RnA_element = original_author_wrapper.find_element_by_xpath('.//span[@data-tooltip-content]')\n",
    "        uc_df['original_author_RnA_text'] = original_author_RnA_element.get_attribute('data-tooltip-content') # Reputability and authenticity \n",
    "        uc_df['original_author_info'] =  original_author_wrapper.find_element_by_xpath('.//div[contains(@class,\"_8yb\")]').text.splitlines()\n",
    "    except:\n",
    "        print(\"Something went wrong when trying to figure out which type of content this author shared\")\n",
    "        pass\n",
    "                        \n",
    "    try:\n",
    "        # Original content privacy info, text and time\n",
    "        original_content_part = content_part.find_element_by_xpath('.//div[contains(@class,\"mtm\")]')\n",
    "        uc_df['original_privacy'] = original_content_part.find_element_by_xpath('.//a[contains(@class,\"Privacy\")]').get_attribute('aria-label')\n",
    "        uc_df['original_text'] = original_content_part.find_element_by_xpath('.//div[contains(@class,\"mtm _5pco\")]').text\n",
    "        uc_df['original_time'] = content_part.find_element_by_xpath('.//abbr[@data-utime]').get_attribute('data-utime')\n",
    "    except:\n",
    "        print(\"Couldn't find original content privacy or text or utime\")\n",
    "\n",
    "    \n",
    "    # Location \n",
    "    try:\n",
    "        uc_df['location'] = get_text_excluding_children(browser,original_content_part.find_elements_by_xpath('.//a[@class=\"_5pcq\"]')[1])\n",
    "        uc_df['location_link'] = original_content_part.find_elements_by_xpath('.//a[@class=\"_5pcq\"]')[1].get_attribute('href')\n",
    "    except:\n",
    "        print(\"Couldn't find location information\")\n",
    "\n",
    "    return(pd.DataFrame(uc_df,index=['information']).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"srt_monitor\">SRT Monitor</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greetings!\n",
    "\n",
    "We have finally arrived here, I'm so glad that I made this far, Ugh!\n",
    "\n",
    "Now let's finally monitor my daily life and store the information, YAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interaction (UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asssigning Skip/Do not skip:\n",
    "- 0 - Skip - Not Valid:\n",
    "    - skip(0)\n",
    "\n",
    "- 1 - Do not skip - valid\n",
    "    - skip(1)\n",
    "\n",
    "## Asssigning Relevance:\n",
    "\n",
    "- 1 - Not Useful/Off-Topic\n",
    "    - relevancce(1)\n",
    "\n",
    "- 2 - Reasonable Match\n",
    "    - relevance(2)\n",
    "\n",
    "- 3 - Primary Match\n",
    "    - relevance(3)\n",
    "\n",
    "## Asssigning Confidence:\n",
    "\n",
    "- 1 - Not confident:\n",
    "     - confidence(1)\n",
    "\n",
    "- 2 - Somewhat confident:\n",
    "    - confidence(2)\n",
    "\n",
    "- 3 - Confident:\n",
    "    - confidence(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Click on submit button\n",
    "def submit():\n",
    "    return browser.find_element_by_xpath(\"//button[contains(text(),'Submit')]\").click()\n",
    "\n",
    "# Choose Skip Category \n",
    "def set_skip(label):\n",
    "\n",
    "    if label == 0:\n",
    "        # 0 - Skip - not valid\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[0].click()\n",
    "    elif label == 1:\n",
    "    # 1 - Do not skip - valid\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[1].click()\n",
    "    else:\n",
    "        raise Exception(\"Label must be boolean: 0 - Skip - not valid OR 1 - Do not skip - valid\")\n",
    "\n",
    "# Choose Relevance Category \n",
    "def set_relevance(label):\n",
    "    if label == 1:\n",
    "        #1 - Not Useful/Off-Topic\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[2].click()\n",
    "    elif label == 2:\n",
    "        # 2 - Reasonable Match\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[3].click()\n",
    "    elif label == 3:\n",
    "        # 3 - Primary Match\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[4].click()\n",
    "    else:\n",
    "        raise Exception(\"Label must be integer in range [0,3]: 1 - Not Useful/Off-Topic; 2 - Reasonable Match OR 3 - Primary Match\")\n",
    "\n",
    "# Choose Confidence Category\n",
    "def set_confidence(label):\n",
    "    if label == 1:\n",
    "        # 1 - Not confident\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[5].click()\n",
    "    elif label ==2:\n",
    "        # 2 - Somewhat confident\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[6].click()\n",
    "    elif label ==3:\n",
    "        # 3 - Confident\n",
    "        browser.find_elements_by_css_selector(\"._5ncw\")[7].click()\n",
    "    else:\n",
    "        raise Exception(\"Label must be integer in range [0,3]: 1 - Not confident; 2 - Somewhat confident OR 3 - Confident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def current_screen():\n",
    "    return(Image.open(io.BytesIO(browser.get_screenshot_as_png())).convert('LA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratings_image_list():\n",
    "    # Get Rating Elements\n",
    "    ratings = browser.find_elements_by_css_selector(\"._5ncw\")\n",
    "\n",
    "    # Get Rating coordinates (x,y) and size (height,width)\n",
    "    coordinates = [ratings[i].location for i in range(0,len(ratings))]\n",
    "    size = [ratings[i].size for i in range(0,len(ratings))]\n",
    "\n",
    "    # Create a list with ratings images\n",
    "    ratings_list = []\n",
    "    for i in range(0,len(ratings)):\n",
    "        x,y,h,w = coordinates[i]['x'],coordinates[i]['y'],size[i]['height'],size[i]['width']\n",
    "        ratings_list.append(current_screen().crop(box=(x,y,x+w,y+h)))\n",
    "    return(ratings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_all_ratings_from_list(ratings_list):\n",
    "    # Print Ratings\n",
    "    for e in ratings_list:\n",
    "        plt.figure()\n",
    "        plt.imshow(e)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_filled(checkbox_image):\n",
    "    true_value = 181 # First element which differs in index 86\n",
    "    false_value = 229 # First element which differs in index 86\n",
    "    comparison = np.asarray(checkbox_image)\n",
    "    comparison = comparison.reshape(comparison.size,-1)\n",
    "    if comparison[86].item() == true_value:\n",
    "        return(True)\n",
    "    elif comparison[86].item() == false_value:\n",
    "        return(False)\n",
    "    else:\n",
    "        print(\"Something went wrong with this function, sorry but a better classifier is needed\")\n",
    "        return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decisions_as_list():\n",
    "    if get_ratings_image_list():\n",
    "        decisions = []\n",
    "        for e in get_ratings_image_list():\n",
    "            decisions.append(is_filled(e))\n",
    "        return(decisions)\n",
    "    else:\n",
    "        return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decisions_as_dict():\n",
    "    dictx = {}\n",
    "    if browser.find_elements_by_css_selector(\".pvs\"):\n",
    "        labels = browser.find_elements_by_css_selector(\".pvs\")\n",
    "    listx = [e.text for e in labels]\n",
    "    decisions = get_decisions_as_list()\n",
    "    for i,e in enumerate(listx):\n",
    "        dictx[e] = decisions[i]\n",
    "    return(dictx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decisions_textboxes():\n",
    "    return([e.text for e in browser.find_elements_by_xpath('//textarea[@placeholder=\"Enter reason here\"]')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Content (Hoverables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_current_job(driver):\n",
    "    if not os.path.exists('jobs'):\n",
    "        os.makedirs('jobs')\n",
    "    files = [f for f in os.listdir('./jobs/')]\n",
    "    index_matches = [i for i, s in enumerate(files) if (get_job_internal_info_as_dict()['Job ID'].encode('utf-8') in s\n",
    "                                                        and 'labels' not in s)]\n",
    "    if index_matches:\n",
    "        matches = [files[i] for i in index_matches]\n",
    "        dashes_index = [e.find('-') for e in matches]\n",
    "        matched_index = pd.Series([int(matches[i][dash_index+1:-4]) for i,dash_index in enumerate(dashes_index)])\n",
    "        filename = get_job_internal_info_as_dict()['Job ID'].encode('utf-8') + '-' + str(matched_index.max() + 1) + '.csv'\n",
    "        # Check missing files\n",
    "        for i in range(0,matched_index.max()):\n",
    "            current_name = get_job_internal_info_as_dict()['Job ID'].encode('utf-8') + '-' + str(i) + '.csv'\n",
    "            if current_name not in matches:\n",
    "                missing_files.append(current_name)\n",
    "    else:\n",
    "        filename = get_job_internal_info_as_dict()['Job ID'].encode('utf-8') + '-0.csv'\n",
    "    print('Adopted filename: {}'.format(filename))\n",
    "    missing_files =[]\n",
    "    print('Missing files: {}'.format(missing_files))\n",
    "    # Saving job information\n",
    "    df = get_all_webelements_information_as_pd_dataframe(driver).drop(['parent','webelement'],axis=1)\n",
    "    df.to_csv('./jobs/{}'.format(filename), sep=',', encoding='utf-8')\n",
    "    # Saving Labels\n",
    "    labels = pd.DataFrame(get_decisions_as_dict().items(),columns=['labels','decision'])\n",
    "    labels = labels.set_index(labels['labels'])\n",
    "    del labels['labels']\n",
    "    labels = labels.transpose()\n",
    "    labels.to_csv('{}{}{}'.format(\".\\jobs\\\\\", filename[0:-4],'_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likely Misstypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing Consecutive Double Letters\n",
    "def remove_consecutive_double_letters(string):\n",
    "    return(''.join(c for c, _ in groupby(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_matches(result):\n",
    "    # Not an exact match If friends like the page part\n",
    "    result_tokenized = word_tokenize(result)\n",
    "    query_tokenized = word_tokenize(query_info_dict()['query'])\n",
    "    # Matching elements\n",
    "    matches = [x for x in query_tokenized if x in result_tokenized]\n",
    "    missing_elements = []\n",
    "    if len(matches) != len(query_tokenized):\n",
    "        # Missing Elements\n",
    "        missing_elements = query_tokenized.copy()\n",
    "        for match in matches:\n",
    "            missing_elements.remove(match)\n",
    "        # partial group/page name matches  with no social engagement  should  be rated as  OFF-TOPIC. \n",
    "        \n",
    "    return({\"matches\": matches,'missing_elements': missing_elements})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row0 = [str(number % 10) for number in range(1,11)]\n",
    "row1 = ['q','w','e','r','t','y','u','i','o','p','Â´','[']\n",
    "row2 = ['a','s','d','f','g','h','j','k','l','Ã§','~',']']\n",
    "row3 = ['z','x','c','v','b','n','m',',','.',';']\n",
    "abnt = [row0,row1,row2,row3]\n",
    "\n",
    "likely_misstypes = collections.defaultdict(dict)\n",
    "\n",
    "for i,row in enumerate(abnt):\n",
    "    \n",
    "    # Generating a dictionary with all up-down adjacent letters to misstype\n",
    "    up_row = down_row = None\n",
    "    if i > 0:\n",
    "        up_row = abnt[i-1]\n",
    "    if i < len(abnt) - 1:\n",
    "        down_row = abnt[i+1]\n",
    "\n",
    "    for j,letter in enumerate(row):\n",
    "        # Generating a dictionary with all left-right adjacent letters to misstype\n",
    "        left_adjacent = right_adjacent = []\n",
    "        if j > 0:\n",
    "            left_adjacent = [row[j-1]]\n",
    "        if j < len(row) - 1:\n",
    "            right_adjacent = [row[j+1]]\n",
    "        \n",
    "        # Generating a dictionary with all up-down adjacent letters to misstype\n",
    "        up_adjacent = down_adjacent = []\n",
    "        if j == 0:\n",
    "            add = 1\n",
    "            sub = -1\n",
    "        else:\n",
    "            add = 1\n",
    "            sub = 1\n",
    "        if up_row:\n",
    "            if j <= len(up_row) - 1:\n",
    "                up_adjacent = []\n",
    "                up_adjacent.append(up_row[j])\n",
    "                if j + 1 < len(up_row):\n",
    "                    up_adjacent.append(up_row[j+add])\n",
    "        if down_row:\n",
    "            if j <= len(down_row) - 1:\n",
    "                down_adjacent = []\n",
    "                down_adjacent.append(down_row[j])\n",
    "                if j + 1 < len(down_row) - 1:\n",
    "                    down_adjacent.append(down_row[j-sub])\n",
    "        \n",
    "        likely_misstypes[letter] = {'left': left_adjacent,'right': right_adjacent,\n",
    "                                'up': up_adjacent,'down': down_adjacent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_fonetic_equivalence = {'s': ['x','z','Ã§','ss','sc','xc'],\n",
    "                                'x': ['s','z','cs','ks','ch'],\n",
    "                                'z': ['s','x'],\n",
    "                                'e': ['i'],\n",
    "                                'i': ['e','y'],\n",
    "                                'y': ['i'],\n",
    "                                'r': ['rr'],\n",
    "                                'Ã§': ['s','ss','c','sc'],\n",
    "                                'c': ['k','Ã§','q','Ã§'],\n",
    "                                'k': ['c','q'],\n",
    "                                'q': ['q','c'],\n",
    "                                'f': ['ph'],\n",
    "                                'l': ['ll','u'],\n",
    "                                'u': ['l'],\n",
    "                                'll': ['l'],\n",
    "                                'ph': ['f'],\n",
    "                                'ss': ['s','Ã§','sc','xc'],\n",
    "                                'sc': ['s','Ã§','ss','xc'],\n",
    "                                'cs': ['x','ks'],\n",
    "                                'xc': ['s','ss','sc'],\n",
    "                                'ks': ['cs','x'],\n",
    "                                'ch': ['x'],\n",
    "                                'lo': ['lou'], #ex Lourdes, Lurdes.\n",
    "                                'lu': ['lou'],\n",
    "                                'lou': ['lu','lo'],\n",
    "                                'rr': ['r']}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
